{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f270bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import json\n",
    "import munch\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "import munch\n",
    "workdir = os.getcwd()+\"/\"\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.get_device_details(gpus[0])\n",
    "sys.path.insert(0, workdir+'/DeepBSDE')\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(linewidth=200)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.3g}'.format\n",
    "sns.set(font_scale = 1.0, rc={\"grid.linewidth\": 1,'grid.color': '#b0b0b0', 'axes.edgecolor': 'black',\"lines.linewidth\": 3.0}, style = 'whitegrid')\n",
    "\n",
    "# import DGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1fbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim (int):       dimensionality of input data\n",
    "            output_dim (int):      number of outputs for LSTM layers\n",
    "            trans1, trans2 (str):  activation functions used inside the layer; \n",
    "                                   one of: \"tanh\" (default), \"relu\" or \"sigmoid\"\n",
    "        \n",
    "        Returns: customized Keras layer object used as intermediate layers in DGM\n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of LSTMLayer)\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        # add properties for layer including activation functions used inside the layer  \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        ### define LSTM layer parameters (use Xavier initialization)\n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    # main function to be called \n",
    "    def call(self, S, X):\n",
    "        '''Compute output of a LSTMLayer for a given inputs S,X .    \n",
    "\n",
    "        Args:            \n",
    "            S: output of previous layer\n",
    "            X: data input\n",
    "        \n",
    "        Returns: customized Keras layer object used as intermediate layers in DGM\n",
    "        '''   \n",
    "        \n",
    "        # compute components of LSTM layer output (note H uses a separate activation function)\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        # compute LSTM layer output\n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new\n",
    "\n",
    "#%% Fully connected (dense) layer - modification of Keras layer class\n",
    "   \n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim:       dimensionality of input data\n",
    "            output_dim:      number of outputs for dense layer\n",
    "            transformation:  activation function used inside the layer; using\n",
    "                             None is equivalent to the identity map \n",
    "        \n",
    "        Returns: customized Keras (fully connected) layer object \n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of DenseLayer)\n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        ### define dense layer parameters (use Xavier initialization)\n",
    "        # w vectors (weighting vectors for output of previous layer)\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.keras.initializers.glorot_normal)\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    # main function to be called \n",
    "    def call(self,X):\n",
    "        '''Compute output of a dense layer for a given input X \n",
    "\n",
    "        Args:                        \n",
    "            X: input to layer            \n",
    "        '''\n",
    "        \n",
    "        # compute dense layer output\n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S\n",
    "\n",
    "#%% Neural network architecture used in DGM - modification of Keras Model class\n",
    "    \n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        '''\n",
    "        Args:\n",
    "            layer_width: \n",
    "            n_layers:    number of intermediate LSTM layers\n",
    "            input_dim:   spaital dimension of input data (EXCLUDES time dimension)\n",
    "            final_trans: transformation used in final layer\n",
    "        \n",
    "        Returns: customized Keras model object representing DGM neural network\n",
    "        '''  \n",
    "        \n",
    "        # create an instance of a Model object (call initialize function of superclass of DGMNet)\n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        # define initial layer as fully connected \n",
    "        # NOTE: to account for time inputs we use input_dim+1 as the input dimensionality\n",
    "#         self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim, transformation = \"tanh\")\n",
    "        \n",
    "        # define intermediate LSTM layers\n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "#             self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1))\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim))\n",
    "        \n",
    "        # define final layer as fully connected with a single output (function value)\n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    \n",
    "    # main function to be called  \n",
    "    def call(self,X):\n",
    "        '''            \n",
    "        Args:\n",
    "            t: sampled time inputs \n",
    "            x: sampled space inputs\n",
    "\n",
    "        Run the DGM model and obtain fitted function value at the inputs (t,x)                \n",
    "        '''  \n",
    "        \n",
    "        # define input vector as time-space pairs\n",
    "#         X = tf.concat([t,x],1)\n",
    "\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933e4a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 15:58:21.433685: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 15:58:21.985257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 324 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:2f:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "delta = tf.constant(0.002, dtype = tf.float64)\n",
    "phi1 = tf.constant(28.0, dtype = tf.float64)\n",
    "phi2 = tf.constant(28.0, dtype = tf.float64)\n",
    "beta1 = tf.constant(0.01, dtype = tf.float64)\n",
    "beta2 = tf.constant(0.01, dtype = tf.float64)\n",
    "eta1 = tf.constant(0.012790328319261378, dtype = tf.float64)\n",
    "eta2 = tf.constant(0.012790328319261378, dtype = tf.float64)\n",
    "a11 = tf.constant(0.014, dtype = tf.float64)\n",
    "alpha = tf.constant(0.1, dtype = tf.float64)\n",
    "scale = np.sqrt(1.754)\n",
    "sigma_k1 = tf.convert_to_tensor(scale*np.array([[.00477,               .0,   .0]]), dtype = tf.float64)\n",
    "sigma_k2 = tf.convert_to_tensor(scale*np.array([[.0              , .00477,   .0]]), dtype = tf.float64)\n",
    "sigma_z =  tf.convert_to_tensor(np.array([[.011*np.sqrt(.5)   , .011*np.sqrt(.5)   , .025]]), dtype = tf.float64)\n",
    "\n",
    "clowerlim = tf.constant(0.0001, dtype = tf.float64)\n",
    "rho = tf.constant(1.0, dtype = tf.float64)\n",
    "gamma = tf.constant(8.0, dtype = tf.float64)\n",
    "kappa = tf.constant(4.0, dtype = tf.float64)\n",
    "zeta = tf.constant(0.5, dtype = tf.float64)\n",
    "wMin = tf.constant(-1., dtype = tf.float64)\n",
    "wMax = tf.constant(1., dtype = tf.float64)\n",
    "zMin = tf.constant(-1., dtype = tf.float64)\n",
    "zMax = tf.constant(1., dtype = tf.float64)\n",
    "\n",
    "\n",
    "params = {'delta':delta,\n",
    "         'phi1':phi1,\n",
    "         'phi2':phi2,\n",
    "         'beta1':beta1,\n",
    "         'beta2':beta2,\n",
    "         'eta1':eta1,\n",
    "         'eta2':eta2,\n",
    "         'a11':a11,\n",
    "         'alpha':alpha,\n",
    "         'sigma_k1':sigma_k1,\n",
    "         'sigma_k2':sigma_k2,\n",
    "         'sigma_z':sigma_z,\n",
    "         'rho':rho,\n",
    "         'gamma':gamma,\n",
    "         'kappa':kappa,\n",
    "         'zeta':zeta,\n",
    "         'wMin':wMin,\n",
    "         'wMax':wMax,\n",
    "         'zMin':zMin,\n",
    "         'zMax':zMax,\n",
    "         'clowerlim':clowerlim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a2d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def calder(func,W, Z):\n",
    "    X = tf.concat([W,Z], axis=1)\n",
    "    logXiE       = func(X)\n",
    "    \n",
    "    dW_logXiE     = tf.gradients(logXiE, W)[0];       dZ_logXiE     = tf.gradients(logXiE, Z)[0];      \n",
    "    dW2_logXiE    = tf.gradients(dW_logXiE, W)[0];    dZ2_logXiE    = tf.gradients(dZ_logXiE, Z)[0];    \n",
    "    dWdZ_logXiE   = tf.gradients(dW_logXiE, Z)[0];     \n",
    "    \n",
    "    return dW_logXiE, dZ_logXiE, dW2_logXiE, dZ2_logXiE, dWdZ_logXiE\n",
    "\n",
    "@tf.function \n",
    "def calvar(func, W, Z, params):\n",
    "    X = tf.concat([W,Z], axis=1)\n",
    "    logXiE       = func(X)\n",
    "    Vr, Vz, Vrr, Vzz,  Vrz = calder(func, W, Z)\n",
    "\n",
    "    k1a = (1-params['zeta'] + params['zeta']*tf.exp(W*(1-params['kappa'])))**(1/(params['kappa']-1))\n",
    "    k2a = ((1-params['zeta'])*tf.exp(W*(params['kappa']-1)) + params['zeta'])**(1/(params['kappa']-1))\n",
    "\n",
    "    bb1 = (params['alpha'] - 1/params['phi1']*k1a-1/params['phi2']*k2a)\n",
    "    cc1 = params['delta']*k1a**2/params['phi1']/((1-params['zeta'])*k1a**(1-params['kappa'])-Vr) + params['delta']*k2a**2/params['phi2']/(params['zeta']*(k2a)**(1-params['kappa'])+Vr)\n",
    "    aa1 = -1\n",
    "    sqrt_test1 = bb1**2 - 4*aa1*cc1;\n",
    "    event_A = tf.cast((sqrt_test1 >= 0),tf.float64);\n",
    "\n",
    "    c_root_large = event_A * (-bb1 - tf.sqrt(event_A*sqrt_test1))/(2*aa1);\n",
    "    c_root_small = event_A * (-bb1 + tf.sqrt(event_A*sqrt_test1))/(2*aa1);\n",
    "    c_root_large = c_root_large*tf.cast(tf.math.greater(c_root_large,params['clowerlim']),tf.float64) + \\\n",
    "        params['clowerlim']*tf.ones([batchSize,1],tf.float64)*tf.cast(tf.math.less(c_root_large,params['clowerlim']),tf.float64)\n",
    "\n",
    "    d1_root = 1/params['phi1']-params['delta']*k1a/((1-params['zeta'])*k1a**(1-params['kappa'])-Vr)/params['phi1']/c_root_large\n",
    "    d2_root = 1/params['phi2']-params['delta']*k2a/(params['zeta']*(k2a)**(1-params['kappa'])+Vr)/params['phi2']/c_root_large\n",
    "\n",
    "    uu = params['delta']*tf.math.log(c_root_large)\n",
    "    \n",
    "    mu_k1 = (d1_root - params['phi1']/2*d1_root**2) + params['beta1']*Z - eta1\n",
    "    mu_k2 = (d2_root - params['phi2']/2*d2_root**2) + params['beta2']*Z - eta2\n",
    "    mu_r = mu_k2 - mu_k1 - 1/2*(tf.reduce_sum(params['sigma_k2']**2) - tf.reduce_sum(params['sigma_k1']**2))\n",
    "\n",
    "    dkadk1dk1 = (params['kappa']-1)*(1-params['zeta'])**2*(k1a)**(-2*params['kappa']+2) - params['kappa']*(1-params['zeta'])*(k1a)**(-params['kappa']+1)\n",
    "    dkadk1dk2 = (params['kappa']-1)*params['zeta']*(1-params['zeta'])*(k1a)**(-params['kappa']+1)*(k2a)**(-params['kappa']+1)\n",
    "    dkadk2dk2 = (params['kappa']-1)*params['zeta']**2*(k2a)**(-2*params['kappa']+2) - params['kappa']*(1-params['zeta'])*(k2a)**(-params['kappa']+1)\n",
    "\n",
    "    mu_1 = mu_k1*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ \\\n",
    "            mu_k2*(params['zeta'])*(k2a)**(1-params['kappa'])+ \\\n",
    "            1/2*(tf.reduce_sum(params['sigma_k1']**2)*dkadk1dk1 + \\\n",
    "                                                tf.reduce_sum(params['sigma_k2']**2)*dkadk2dk2 + \\\n",
    "                                                2*tf.reduce_sum(params['sigma_k1']*params['sigma_k2'])*dkadk1dk2)\n",
    "\n",
    "    zdrift = -params['a11']*Z\n",
    "\n",
    "    h1 = params['sigma_k1'][0,0]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,0]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,0]-params['sigma_k1'][0,0])*Vr + params['sigma_z'][0,0]*Vz\n",
    "    \n",
    "    h2 = params['sigma_k1'][0,1]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,1]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,1]-params['sigma_k1'][0,1])*Vr + params['sigma_z'][0,1]*Vz\n",
    "    \n",
    "    hz = params['sigma_k1'][0,2]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,2]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,2]-params['sigma_k1'][0,2])*Vr + params['sigma_z'][0,2]*Vz\n",
    "\n",
    "    penalty_term = (1-params['gamma'])*(h1**2 + h2**2 + hz**2)/2\n",
    "    uu = uu + penalty_term + mu_1\n",
    "    t1 = tf.reduce_sum((params['sigma_k2']-params['sigma_k1'])**2)\n",
    "    t2 = tf.reduce_sum((params['sigma_k2']-params['sigma_k1'])*params['sigma_z'])*2\n",
    "    t3 = tf.reduce_sum(params['sigma_z']**2)\n",
    "\n",
    "    higordder = (t1*Vrr + t2*Vrz + t3*Vzz)/2\n",
    "    \n",
    "    return logXiE, k1a, k2a, c_root_large, d1_root, d2_root, uu, penalty_term, mu_1, mu_k1, mu_k2, mu_r, zdrift, h1, h2, hz, higordder, Vr, Vz, Vrr, Vzz, Vrz\n",
    "    \n",
    "@tf.function \n",
    "def HJB_loss_E(func, W, Z, params):\n",
    "    X = tf.concat([W,Z], axis=1)\n",
    "    logXiE, k1a, k2a, c_root_large, d1_root, d2_root, uu, penalty_term, mu_1, mu_k1, mu_k2, mu_r, zdrift, h1, h2, hz, higordder, Vr, Vz, Vrr, Vzz, Vrz = calvar(func, W, Z, params)\n",
    "    HJB = uu + mu_r*Vr + zdrift*Vz + higordder - params['delta']*logXiE\n",
    "    \n",
    "    return HJB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8411b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_factory(model, loss, valueFunctionLogE, W, Z, params, loss_type, targets):\n",
    "\n",
    "    ## Obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.product(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # Create a function that will compute the value and gradient. This can be the function that the factory returns\n",
    "    @tf.function\n",
    "    def val_and_grad(params_1d):\n",
    "        with tf.GradientTape() as tape:\n",
    "          ## Update the parameters in the model\n",
    "            assign_new_model_parameters(params_1d)\n",
    "            ## Calculate the loss \n",
    "            loss_value = loss_type(loss(valueFunctionLogE, W, Z, params), targets)\n",
    "        ## Calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "#         tf.print('grads', grads[-1])\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "        del tape\n",
    "\n",
    "        ## Print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "#         tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        ## Store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "        return loss_value, grads\n",
    "\n",
    "    def f(params_1d):\n",
    "      return [vv.numpy().astype(np.float64)  for vv in val_and_grad(params_1d)]\n",
    "\n",
    "    ## Store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f\n",
    "  \n",
    "\n",
    "## Training step BFGS\n",
    "def training_step_BFGS(valueFunctionLogE, W, Z, params, targets, maxiter, maxfun, gtol, maxcor, maxls, ftol):\n",
    "\n",
    "  ## Train experts NN\n",
    "  loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "  func_E = function_factory(valueFunctionLogE, HJB_loss_E, valueFunctionLogE, W, Z, params, loss_fun, targets)\n",
    "  init_params_E = tf.dynamic_stitch(func_E.idx, valueFunctionLogE.trainable_variables)\n",
    "\n",
    "  start = time.time()\n",
    "  results = optimize.minimize(func_E, x0 = init_params_E.numpy(), method = 'L-BFGS-B', jac = True, options = {'maxiter': maxiter, 'maxfun': maxfun, 'gtol': gtol, 'maxcor': maxcor, 'maxls': maxls, 'ftol' : ftol})\n",
    "  end = time.time()\n",
    "  print('Elapsed time for experts {:.4f} sec'.format(end - start))\n",
    "  # after training, the final optimized parameters are still in results.position\n",
    "  # so we have to manually put them back to the model\n",
    "  func_E.assign_new_model_parameters(results.x)\n",
    " \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6757c55d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "0.0034132270176228407\n",
      "Iteration 1\n",
      "0.0034132270176228407\n",
      "Iteration 2\n",
      "0.0034132270176228407\n",
      "Iteration 3\n",
      "0.0034132270176228407\n",
      "Iteration 4\n",
      "0.0034132270176228407\n",
      "Iteration 5\n",
      "0.0034132270176228407\n",
      "Iteration 6\n",
      "0.0034132270176228407\n",
      "Iteration 7\n",
      "0.0034132270176228407\n",
      "Iteration 8\n",
      "0.0034132270176228407\n",
      "Iteration 9\n",
      "0.0034132270176228407\n",
      "Iteration 10\n",
      "0.0034132270176228407\n",
      "Iteration 11\n",
      "0.0034132270176228407\n",
      "Iteration 12\n",
      "0.0034132270176228407\n",
      "Iteration 13\n",
      "0.0034132270176228407\n",
      "Iteration 14\n",
      "0.0034132270176228407\n",
      "Iteration 15\n",
      "0.0034132270176228407\n",
      "Iteration 16\n",
      "0.0034132270176228407\n",
      "Iteration 17\n",
      "0.0034132270176228407\n",
      "Iteration 18\n",
      "0.0034132270176228407\n",
      "Iteration 19\n",
      "0.0034132270176228407\n",
      "Iteration 20\n",
      "0.0034132270176228407\n",
      "Iteration 21\n",
      "0.0034132270176228407\n",
      "Iteration 22\n",
      "0.0034132270176228407\n",
      "Iteration 23\n",
      "0.0034132270176228407\n",
      "Iteration 24\n",
      "0.0034132270176228407\n",
      "Iteration 25\n",
      "0.0034132270176228407\n",
      "Iteration 26\n",
      "0.0034132270176228407\n",
      "Iteration 27\n",
      "0.0034132270176228407\n",
      "Iteration 28\n",
      "0.0034132270176228407\n",
      "Iteration 29\n",
      "0.0034132270176228407\n",
      "Iteration 30\n",
      "0.0034132270176228407\n",
      "Iteration 31\n",
      "0.0034132270176228407\n",
      "Iteration 32\n",
      "0.0034132270176228407\n",
      "Iteration 33\n",
      "0.0034132270176228407\n",
      "Iteration 34\n",
      "0.0034132270176228407\n",
      "Iteration 35\n",
      "0.0034132270176228407\n",
      "Iteration 36\n",
      "0.0034132270176228407\n",
      "Iteration 37\n",
      "0.0034132270176228407\n",
      "Iteration 38\n",
      "0.0034132270176228407\n",
      "Iteration 39\n",
      "0.0034132270176228407\n",
      "Iteration 40\n",
      "0.0034132270176228407\n",
      "Iteration 41\n",
      "0.0034132270176228407\n",
      "Iteration 42\n",
      "0.0034132270176228407\n",
      "Iteration 43\n",
      "0.0034132270176228407\n",
      "Iteration 44\n",
      "0.0034132270176228407\n",
      "Iteration 45\n",
      "0.0034132270176228407\n",
      "Iteration 46\n",
      "0.0034132270176228407\n",
      "Iteration 47\n",
      "0.0034132270176228407\n",
      "Iteration 48\n",
      "0.0034132270176228407\n",
      "Iteration 49\n",
      "0.0034132270176228407\n",
      "Elapsed time for training 0.0296 sec\n"
     ]
    }
   ],
   "source": [
    "points_size = 1\n",
    "dimension = 2\n",
    "units = 16\n",
    "activation = 'tanh'\n",
    "kernel_initializer = 'glorot_normal'\n",
    "batchSize = 1024 * points_size\n",
    "batchSize = 50\n",
    "\n",
    "iter_num = 50\n",
    "BFGS_maxiter  = 50\n",
    "BFGS_maxfun   = 50\n",
    "BFGS_gtol     = 1.0 * np.finfo(float).eps\n",
    "BFGS_maxcor   = 100\n",
    "BFGS_maxls    = 200\n",
    "BFGS_ftol     = 1.0 * np.finfo(float).eps\n",
    "\n",
    "## NN structure\n",
    "tf.keras.backend.set_floatx(\"float64\") ## Use float64 by default\n",
    "\n",
    "logXiE_NN = tf.keras.Sequential(\n",
    "    [tf.keras.Input(shape=[dimension,]),\n",
    "      tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "      tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "      tf.keras.layers.Dense(1,  activation= None,  kernel_initializer='glorot_normal')])\n",
    "# logXiE_NN = DGMNet(layer_width =16, n_layers = 3, input_dim = 2, final_trans=None)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "targets = tf.zeros(shape=(batchSize,1), dtype=tf.float64)\n",
    "for iter in range(iter_num):\n",
    "  W = tf.random.uniform(shape = (batchSize,1), minval = params['wMin'], maxval = params['wMax'], dtype=tf.float64)\n",
    "  Z = tf.random.uniform(shape = (batchSize,1), minval = params['zMin'], maxval = params['zMax'], dtype=tf.float64)\n",
    "#   W = tf.random.normal(shape = (batchSize,1), mean = 0.0, stddev = 0.5, dtype=tf.float64)\n",
    "#   Z = tf.random.normal(shape = (batchSize,1), mean = 0.0, stddev = 0.5, dtype=tf.float64)\n",
    "  print('Iteration', iter)\n",
    "#   results = training_step_BFGS(logXiE_NN, W, Z, params, targets,\\\n",
    "#                     maxiter = BFGS_maxiter, maxfun = BFGS_maxfun, gtol = BFGS_gtol, maxcor = BFGS_maxcor, maxls = BFGS_maxls, ftol = BFGS_ftol)\n",
    "  print(results.fun)\n",
    "  if results.fun< 1e-11:\n",
    "    break\n",
    "end = time.time()\n",
    "training_time = '{:.4f}'.format((end - start)/60)\n",
    "print('Elapsed time for training {:.4f} sec'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "288a958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def lossfun():\n",
    "    loss_type = tf.keras.losses.MeanSquaredError()\n",
    "#     loss = loss_type(HJB_loss_E(logXiE_NN, W, Z, params), targets)\n",
    "    loss = tf.reduce_mean(tf.square(HJB_loss_E(logXiE_NN, W, Z, params)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44a8956f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.name is undefined when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    439\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1317\u001b[0m, in \u001b[0;36m_EagerTensorBase.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1317\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1318\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.name is undefined when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Tensor.name is undefined when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "loss.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cfdac60",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.name is undefined when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tp:\n\u001b[1;32m      3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lossfun()\n\u001b[0;32m----> 4\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradientTape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:579\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    576\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_gradients(\n\u001b[1;32m    577\u001b[0m     loss, var_list\u001b[38;5;241m=\u001b[39mvar_list, grad_loss\u001b[38;5;241m=\u001b[39mgrad_loss, tape\u001b[38;5;241m=\u001b[39mtape\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, grads_and_vars, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    650\u001b[0m ):\n\u001b[1;32m    651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:76\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(filtered)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m---> 76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(filtered)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m---> 76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    439\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1317\u001b[0m, in \u001b[0;36m_EagerTensorBase.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1317\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1318\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.name is undefined when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Tensor.name is undefined when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "with tf.GradientTape() as tp:\n",
    "    loss = lossfun()\n",
    "optimizer.minimize(lossfun(), var_list=[W, Z],tape=tf.GradientTape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75f77aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=19262.111866882613>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfun(W, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "939c6ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=19262.111866882613>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfun(W, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d54f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
