{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20504330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 14:56:50.558362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 14:56:50.709569: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-28 14:56:50.742344: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-28 14:56:51.341618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/cudnn-11.2-el8-x86_64/lib64:/software/cuda-11.2-el8-x86_64/extras/CUPTI/lib64:/software/cuda-11.2-el8-x86_64/lib64:/software/python-anaconda-2022.05-el8-x86_64/lib:/software/cudnn-8.3.1-el8-x86_64/lib64\n",
      "2023-03-28 14:56:51.341692: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/cudnn-11.2-el8-x86_64/lib64:/software/cuda-11.2-el8-x86_64/extras/CUPTI/lib64:/software/cuda-11.2-el8-x86_64/lib64:/software/python-anaconda-2022.05-el8-x86_64/lib:/software/cudnn-8.3.1-el8-x86_64/lib64\n",
      "2023-03-28 14:56:51.341698: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import json\n",
    "import munch\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "import munch\n",
    "workdir = os.getcwd()+\"/\"\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.get_device_details(gpus[0])\n",
    "sys.path.insert(0, workdir+'/DeepBSDE')\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(linewidth=200)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.3g}'.format\n",
    "sns.set(font_scale = 1.0, rc={\"grid.linewidth\": 1,'grid.color': '#b0b0b0', 'axes.edgecolor': 'black',\"lines.linewidth\": 3.0}, style = 'whitegrid')\n",
    "\n",
    "# import DGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4499bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, trans1 = \"tanh\", trans2 = \"tanh\"):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim (int):       dimensionality of input data\n",
    "            output_dim (int):      number of outputs for LSTM layers\n",
    "            trans1, trans2 (str):  activation functions used inside the layer; \n",
    "                                   one of: \"tanh\" (default), \"relu\" or \"sigmoid\"\n",
    "        \n",
    "        Returns: customized Keras layer object used as intermediate layers in DGM\n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of LSTMLayer)\n",
    "        super(LSTMLayer, self).__init__()\n",
    "        \n",
    "        # add properties for layer including activation functions used inside the layer  \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        if trans1 == \"tanh\":\n",
    "            self.trans1 = tf.nn.tanh\n",
    "        elif trans1 == \"relu\":\n",
    "            self.trans1 = tf.nn.relu\n",
    "        elif trans1 == \"sigmoid\":\n",
    "            self.trans1 = tf.nn.sigmoid\n",
    "        \n",
    "        if trans2 == \"tanh\":\n",
    "            self.trans2 = tf.nn.tanh\n",
    "        elif trans2 == \"relu\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        elif trans2 == \"sigmoid\":\n",
    "            self.trans2 = tf.nn.relu\n",
    "        \n",
    "        ### define LSTM layer parameters (use Xavier initialization)\n",
    "        # u vectors (weighting vectors for inputs original inputs x)\n",
    "        self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        \n",
    "        # w vectors (weighting vectors for output of previous layer)        \n",
    "        self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
    "                                    initializer = tf.keras.initializers.glorot_normal)\n",
    "        \n",
    "        # bias vectors\n",
    "        self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
    "        self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
    "        self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
    "        self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n",
    "    \n",
    "    \n",
    "    # main function to be called \n",
    "    def call(self, S, X):\n",
    "        '''Compute output of a LSTMLayer for a given inputs S,X .    \n",
    "\n",
    "        Args:            \n",
    "            S: output of previous layer\n",
    "            X: data input\n",
    "        \n",
    "        Returns: customized Keras layer object used as intermediate layers in DGM\n",
    "        '''   \n",
    "        \n",
    "        # compute components of LSTM layer output (note H uses a separate activation function)\n",
    "        Z = self.trans1(tf.add(tf.add(tf.matmul(X,self.Uz), tf.matmul(S,self.Wz)), self.bz))\n",
    "        G = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ug), tf.matmul(S, self.Wg)), self.bg))\n",
    "        R = self.trans1(tf.add(tf.add(tf.matmul(X,self.Ur), tf.matmul(S, self.Wr)), self.br))\n",
    "        \n",
    "        H = self.trans2(tf.add(tf.add(tf.matmul(X,self.Uh), tf.matmul(tf.multiply(S, R), self.Wh)), self.bh))\n",
    "        \n",
    "        # compute LSTM layer output\n",
    "        S_new = tf.add(tf.multiply(tf.subtract(tf.ones_like(G), G), H), tf.multiply(Z,S))\n",
    "        \n",
    "        return S_new\n",
    "\n",
    "#%% Fully connected (dense) layer - modification of Keras layer class\n",
    "   \n",
    "class DenseLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, output_dim, input_dim, transformation=None):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim:       dimensionality of input data\n",
    "            output_dim:      number of outputs for dense layer\n",
    "            transformation:  activation function used inside the layer; using\n",
    "                             None is equivalent to the identity map \n",
    "        \n",
    "        Returns: customized Keras (fully connected) layer object \n",
    "        '''        \n",
    "        \n",
    "        # create an instance of a Layer object (call initialize function of superclass of DenseLayer)\n",
    "        super(DenseLayer,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        ### define dense layer parameters (use Xavier initialization)\n",
    "        # w vectors (weighting vectors for output of previous layer)\n",
    "        self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
    "                                   initializer = tf.keras.initializers.glorot_normal)\n",
    "        \n",
    "        # bias vectors\n",
    "        self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
    "        \n",
    "        if transformation:\n",
    "            if transformation == \"tanh\":\n",
    "                self.transformation = tf.tanh\n",
    "            elif transformation == \"relu\":\n",
    "                self.transformation = tf.nn.relu\n",
    "        else:\n",
    "            self.transformation = transformation\n",
    "    \n",
    "    \n",
    "    # main function to be called \n",
    "    def call(self,X):\n",
    "        '''Compute output of a dense layer for a given input X \n",
    "\n",
    "        Args:                        \n",
    "            X: input to layer            \n",
    "        '''\n",
    "        \n",
    "        # compute dense layer output\n",
    "        S = tf.add(tf.matmul(X, self.W), self.b)\n",
    "                \n",
    "        if self.transformation:\n",
    "            S = self.transformation(S)\n",
    "        \n",
    "        return S\n",
    "\n",
    "#%% Neural network architecture used in DGM - modification of Keras Model class\n",
    "    \n",
    "class DGMNet(tf.keras.Model):\n",
    "    \n",
    "    # constructor/initializer function (automatically called when new instance of class is created)\n",
    "    def __init__(self, layer_width, n_layers, input_dim, final_trans=None):\n",
    "        '''\n",
    "        Args:\n",
    "            layer_width: \n",
    "            n_layers:    number of intermediate LSTM layers\n",
    "            input_dim:   spaital dimension of input data (EXCLUDES time dimension)\n",
    "            final_trans: transformation used in final layer\n",
    "        \n",
    "        Returns: customized Keras model object representing DGM neural network\n",
    "        '''  \n",
    "        \n",
    "        # create an instance of a Model object (call initialize function of superclass of DGMNet)\n",
    "        super(DGMNet,self).__init__()\n",
    "        \n",
    "        # define initial layer as fully connected \n",
    "        # NOTE: to account for time inputs we use input_dim+1 as the input dimensionality\n",
    "#         self.initial_layer = DenseLayer(layer_width, input_dim+1, transformation = \"tanh\")\n",
    "        self.initial_layer = DenseLayer(layer_width, input_dim, transformation = \"tanh\")\n",
    "        \n",
    "        # define intermediate LSTM layers\n",
    "        self.n_layers = n_layers\n",
    "        self.LSTMLayerList = []\n",
    "                \n",
    "        for _ in range(self.n_layers):\n",
    "#             self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim+1))\n",
    "            self.LSTMLayerList.append(LSTMLayer(layer_width, input_dim))\n",
    "        \n",
    "        # define final layer as fully connected with a single output (function value)\n",
    "        self.final_layer = DenseLayer(1, layer_width, transformation = final_trans)\n",
    "    \n",
    "    \n",
    "    # main function to be called  \n",
    "    def call(self,X):\n",
    "        '''            \n",
    "        Args:\n",
    "            t: sampled time inputs \n",
    "            x: sampled space inputs\n",
    "\n",
    "        Run the DGM model and obtain fitted function value at the inputs (t,x)                \n",
    "        '''  \n",
    "        \n",
    "        # define input vector as time-space pairs\n",
    "#         X = tf.concat([t,x],1)\n",
    "\n",
    "        \n",
    "        # call initial layer\n",
    "        S = self.initial_layer.call(X)\n",
    "        \n",
    "        # call intermediate LSTM layers\n",
    "        for i in range(self.n_layers):\n",
    "            S = self.LSTMLayerList[i].call(S,X)\n",
    "        \n",
    "        # call final LSTM layers\n",
    "        result = self.final_layer.call(S)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcec600c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.004432720393194923>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_size = 1\n",
    "dimension = 3\n",
    "units = 16\n",
    "activation = 'tanh'\n",
    "kernel_initializer = 'glorot_normal'\n",
    "batchSize = 2048 * points_size\n",
    "\n",
    "iter_num = 50\n",
    "BFGS_maxiter  = 1000\n",
    "BFGS_maxfun   = 1000\n",
    "BFGS_gtol     = 1.0 * np.finfo(float).eps\n",
    "BFGS_maxcor   = 100\n",
    "BFGS_maxls    = 100\n",
    "BFGS_ftol     = 1.0 * np.finfo(float).eps\n",
    "\n",
    "## NN structure\n",
    "tf.keras.backend.set_floatx(\"float64\") ## Use float64 by default\n",
    "\n",
    "logXiE_NN = tf.keras.Sequential(\n",
    "    [tf.keras.Input(shape=[dimension,]),\n",
    "      tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "      tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "#       tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "      tf.keras.layers.Dense(1,  activation= None,  kernel_initializer='glorot_normal')])\n",
    "\n",
    "targets = tf.zeros(shape=(batchSize,1), dtype=tf.float64)\n",
    "# logXiE_NN = DGMNet(layer_width =16, n_layers = 3, input_dim = 3, final_trans=None)\n",
    "loss_type = tf.keras.losses.MeanSquaredError()\n",
    "loss_type(HJB_loss_E(logXiE_NN, W, Z, V, params), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e8e9314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:110: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:114: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:38: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:40: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:42: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:44: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:48: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:50: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:52: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:54: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:58: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:59: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:60: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471815/1937691871.py:61: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n"
     ]
    }
   ],
   "source": [
    "lsp = DGMNet(layer_width =50, n_layers = 3, input_dim = 3, final_trans=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40cbcb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_size = 1\n",
    "dimension = 3\n",
    "units = 16\n",
    "activation = 'tanh'\n",
    "kernel_initializer = 'glorot_normal'\n",
    "batchSize = 2048 * points_size\n",
    "\n",
    "iter_num = 50\n",
    "BFGS_maxiter  = 1000\n",
    "BFGS_maxfun   = 1000\n",
    "BFGS_gtol     = 1.0 * np.finfo(float).eps\n",
    "BFGS_maxcor   = 100\n",
    "BFGS_maxls    = 100\n",
    "BFGS_ftol     = 1.0 * np.finfo(float).eps\n",
    "\n",
    "W = tf.random.uniform(shape = (batchSize,1), minval = params['wMin'], maxval = params['wMax'], dtype=tf.float64, name='W')\n",
    "Z = tf.random.uniform(shape = (batchSize,1), minval = params['zMin'], maxval = params['zMax'], dtype=tf.float64, name='Z')\n",
    "V = tf.random.uniform(shape = (batchSize,1), minval = params['vMin'], maxval = params['vMax'], dtype=tf.float64, name='V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83938127",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fn = lambda: 3 * x * x+ 2 * y * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af11e451",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtarget_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "target_fn([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f9db4d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`tape` is required when a `Tensor` loss is passed. Received: loss=0.004432720393194923, tape=None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tp:\n\u001b[1;32m      3\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lossfun(W, Z, V)\n\u001b[0;32m----> 4\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:576\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, var_list, grad_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtape\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:615\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# TODO(joshl): Test that we handle weight decay in a reasonable way.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(loss) \u001b[38;5;129;01mand\u001b[39;00m tape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    616\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tape` is required when a `Tensor` loss is passed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, tape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m tape \u001b[38;5;241m=\u001b[39m tape \u001b[38;5;28;01mif\u001b[39;00m tape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape()\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(loss):\n",
      "\u001b[0;31mValueError\u001b[0m: `tape` is required when a `Tensor` loss is passed. Received: loss=0.004432720393194923, tape=None."
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "with tf.GradientTape() as tp:\n",
    "    loss = lossfun(W, Z, V)\n",
    "optimizer.minimize(loss, var_list=[W, Z, V])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06defb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.533436775]\n",
      "[1.0605762]\n"
     ]
    }
   ],
   "source": [
    "x_train = [1, 2, 3]\n",
    "y_train = [1, 2, 3]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([1]), trainable = True, name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), trainable = True, name='bias')\n",
    "\n",
    "@tf.function\n",
    "def cost(W, b):\n",
    "    y_model = W * x_train + b\n",
    "    error = tf.reduce_mean(tf.square(y_train - y_model))\n",
    "    return error\n",
    "\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "trainable_vars = [W,b]\n",
    "\n",
    "epochs = 100 #(or however many iterations you want it to run)\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape() as tp:\n",
    "        #your loss/cost function must always be contained within the gradient tape instantiation\n",
    "        cost_fn = cost(W, b)\n",
    "    gradients = tp.gradient(cost_fn, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "tf.print(W)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be014bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.00021833646>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35de1b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2048, 1), dtype=float64, numpy=\n",
       "array([[-0.04578205],\n",
       "       [-0.10077951],\n",
       "       [-0.00971282],\n",
       "       ...,\n",
       "       [-0.1225914 ],\n",
       "       [-0.01556702],\n",
       "       [-0.00713679]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HJB_loss_E(logXiE_NN, W, Z, V, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec9a8b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.004247072212786333>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfun(W, Z, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93b1509f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.name is undefined when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlossfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGradientTape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:579\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    576\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_gradients(\n\u001b[1;32m    577\u001b[0m     loss, var_list\u001b[38;5;241m=\u001b[39mvar_list, grad_loss\u001b[38;5;241m=\u001b[39mgrad_loss, tape\u001b[38;5;241m=\u001b[39mtape\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, grads_and_vars, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    650\u001b[0m ):\n\u001b[1;32m    651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:76\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(filtered)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m---> 76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(filtered)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m---> 76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    439\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1317\u001b[0m, in \u001b[0;36m_EagerTensorBase.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1317\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1318\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.name is undefined when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Tensor.name is undefined when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "tf.keras.optimizers.Adam().minimize(lossfun(W, Z, V), var_list=[W, Z, V],tape=tf.GradientTape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef8c2531",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def lossfun(W, Z, V):\n",
    "#     loss = loss_type(HJB_loss_E(logXiE_NN, W, Z, V, params), targets)\n",
    "    loss = tf.reduce_mean(tf.square(HJB_loss_E(logXiE_NN, W, Z, V, params)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a389d19e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.name is undefined when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     cost_fn \u001b[38;5;241m=\u001b[39m lossfun(W, Z, V)\n\u001b[1;32m      8\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tp\u001b[38;5;241m.\u001b[39mgradient(cost_fn, trainable_vars)\n\u001b[0;32m----> 9\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py:689\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, grads_and_vars, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    650\u001b[0m ):\n\u001b[1;32m    651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m      RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m     var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:76\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(filtered)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m---> 76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/utils.py:76\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(filtered)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[0;32m---> 76\u001b[0m     variable \u001b[38;5;241m=\u001b[39m ([\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:446\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    438\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    439\u001b[0m   \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;124m    np_config.enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1317\u001b[0m, in \u001b[0;36m_EagerTensorBase.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mname\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1317\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1318\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor.name is undefined when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Tensor.name is undefined when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate=0.01)\n",
    "trainable_vars = [W, Z, V]\n",
    "epochs = 1 #(or however many iterations you want it to run)\n",
    "for _ in range(epochs):\n",
    "    with tf.GradientTape() as tp:\n",
    "        #your loss/cost function must always be contained within the gradient tape instantiation\n",
    "        cost_fn = lossfun(W, Z, V)\n",
    "    gradients = tp.gradient(cost_fn, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6cfa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logXiE_NN.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ed1ded",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloss_type\u001b[49m(HJB_loss_E(logXiE_NN, W, Z, V, params), targets)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_type' is not defined"
     ]
    }
   ],
   "source": [
    "loss_type(HJB_loss_E(logXiE_NN, W, Z, V, params), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303ecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logXiE_NN.fit(tf.concat([W,Z,V],axis=1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2a74115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2048, 3), dtype=float64, numpy=\n",
       "array([[-0.58574578,  0.08084113,  0.11475454],\n",
       "       [-0.19416303, -0.74564927,  1.54673813],\n",
       "       [-0.46386953, -0.88778465,  0.51521316],\n",
       "       ...,\n",
       "       [-0.98030807, -0.86183961,  1.12626014],\n",
       "       [-0.57981374,  0.5516768 ,  0.22570453],\n",
       "       [ 0.26722965,  0.02210576,  0.70258644]])>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([W,Z,V],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9fc0bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:110: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:114: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:38: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:40: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:42: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:44: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:48: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:50: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:52: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:54: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:58: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:59: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:60: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1471119/4226590721.py:61: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m model \u001b[38;5;241m=\u001b[39m DGMNet(nodes_per_layer, num_layers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# tensor placeholders (_tnsr suffix indicates tensors)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# inputs (time, space domain interior, space domain at initial time)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m t_interior_tnsr \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder\u001b[49m(tf\u001b[38;5;241m.\u001b[39mfloat32, [\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    126\u001b[0m S_interior_tnsr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, [\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    127\u001b[0m t_terminal_tnsr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mplaceholder(tf\u001b[38;5;241m.\u001b[39mfloat32, [\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% Parameters \n",
    "\n",
    "# Option parameters\n",
    "r = 0.05           # Interest rate\n",
    "sigma = 0.25       # Volatility\n",
    "K = 50             # Strike\n",
    "T = 1              # Terminal time\n",
    "S0 = 0.5           # Initial price\n",
    "\n",
    "# Solution parameters (domain on which to solve PDE)\n",
    "t_low = 0 + 1e-10    # time lower bound\n",
    "S_low = 0.0 + 1e-10  # spot price lower bound\n",
    "S_high = 2*K         # spot price upper bound\n",
    "\n",
    "# neural network parameters\n",
    "num_layers = 3\n",
    "nodes_per_layer = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training parameters\n",
    "sampling_stages  = 100   # number of times to resample new time-space domain points\n",
    "steps_per_sample = 10    # number of SGD steps to take before re-sampling\n",
    "\n",
    "# Sampling parameters\n",
    "nSim_interior = 1000\n",
    "nSim_terminal = 100\n",
    "S_multiplier  = 1.5   # multiplier for oversampling i.e. draw S from [S_low, S_high * S_multiplier]\n",
    "\n",
    "# Plot options\n",
    "n_plot = 41  # Points on plot grid for each dimension\n",
    "\n",
    "# Save options\n",
    "saveOutput = False\n",
    "saveName   = 'BlackScholes_EuropeanCall'\n",
    "saveFigure = False\n",
    "figureName = 'BlackScholes_EuropeanCall.png'\n",
    "\n",
    "#%% Black-Scholes European call price\n",
    "\n",
    "def BlackScholesCall(S, K, r, sigma, t):\n",
    "    ''' Analytical solution for European call option price under Black-Scholes model \n",
    "    \n",
    "    Args:\n",
    "        S:     spot price\n",
    "        K:     strike price\n",
    "        r:     risk-free interest rate\n",
    "        sigma: volatility\n",
    "        t:     time\n",
    "    ''' \n",
    "    \n",
    "    d1 = (np.log(S/K) + (r + sigma**2 / 2) * (T-t))/(sigma * np.sqrt(T-t))\n",
    "    d2 = d1 - (sigma * np.sqrt(T-t))\n",
    "\n",
    "    callPrice = S * spstats.norm.cdf(d1) - K * np.exp(-r * (T-t)) * spstats.norm.cdf(d2)\n",
    "    \n",
    "    return callPrice\n",
    "\n",
    "#%% Sampling function - randomly sample time-space pairs \n",
    "\n",
    "def sampler(nSim_interior, nSim_terminal):\n",
    "    ''' Sample time-space points from the function's domain; points are sampled\n",
    "        uniformly on the interior of the domain, at the initial/terminal time points\n",
    "        and along the spatial boundary at different time points. \n",
    "    \n",
    "    Args:\n",
    "        nSim_interior: number of space points in the interior of the function's domain to sample \n",
    "        nSim_terminal: number of space points at terminal time to sample (terminal condition)\n",
    "    ''' \n",
    "    \n",
    "    # Sampler #1: domain interior\n",
    "    t_interior = np.random.uniform(low=t_low, high=T, size=[nSim_interior, 1])\n",
    "    S_interior = np.random.uniform(low=S_low, high=S_high*S_multiplier, size=[nSim_interior, 1])\n",
    "\n",
    "    # Sampler #2: spatial boundary\n",
    "        # no spatial boundary condition for this problem\n",
    "    \n",
    "    # Sampler #3: initial/terminal condition\n",
    "    t_terminal = T * np.ones((nSim_terminal, 1))\n",
    "    S_terminal = np.random.uniform(low=S_low, high=S_high*S_multiplier, size = [nSim_terminal, 1])\n",
    "    \n",
    "    return t_interior, S_interior, t_terminal, S_terminal\n",
    "\n",
    "#%% Loss function for Fokker-Planck equation\n",
    "\n",
    "def loss(model, t_interior, S_interior, t_terminal, S_terminal):\n",
    "    ''' Compute total loss for training.\n",
    "    \n",
    "    Args:\n",
    "        model:      DGM model object\n",
    "        t_interior: sampled time points in the interior of the function's domain\n",
    "        S_interior: sampled space points in the interior of the function's domain\n",
    "        t_terminal: sampled time points at terminal point (vector of terminal times)\n",
    "        S_terminal: sampled space points at terminal time\n",
    "    ''' \n",
    "    \n",
    "    # Loss term #1: PDE\n",
    "    # compute function value and derivatives at current sampled points\n",
    "    V = model(t_interior, S_interior)\n",
    "    V_t = tf.gradients(V, t_interior)[0]\n",
    "    V_s = tf.gradients(V, S_interior)[0]\n",
    "    V_ss = tf.gradients(V_s, S_interior)[0]\n",
    "    diff_V = V_t + 0.5 * sigma**2 * S_interior**2 * V_ss + r * S_interior * V_s - r*V\n",
    "\n",
    "    # compute average L2-norm of differential operator\n",
    "    L1 = tf.reduce_mean(tf.square(diff_V)) \n",
    "    \n",
    "    # Loss term #2: boundary condition\n",
    "        # no boundary condition for this problem\n",
    "    \n",
    "    # Loss term #3: initial/terminal condition\n",
    "    target_payoff = tf.nn.relu(S_terminal - K)\n",
    "    fitted_payoff = model(t_terminal, S_terminal)\n",
    "    \n",
    "    L3 = tf.reduce_mean( tf.square(fitted_payoff - target_payoff) )\n",
    "\n",
    "    return L1, L3\n",
    "\n",
    "#%% Set up network\n",
    "\n",
    "# initialize DGM model (last input: space dimension = 1)\n",
    "model = DGMNet(nodes_per_layer, num_layers, 1)\n",
    "\n",
    "# tensor placeholders (_tnsr suffix indicates tensors)\n",
    "# inputs (time, space domain interior, space domain at initial time)\n",
    "t_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "S_interior_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "t_terminal_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "S_terminal_tnsr = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "# loss \n",
    "L1_tnsr, L3_tnsr = loss(model, t_interior_tnsr, S_interior_tnsr, t_terminal_tnsr, S_terminal_tnsr)\n",
    "loss_tnsr = L1_tnsr + L3_tnsr\n",
    "\n",
    "# option value function\n",
    "V = model(t_interior_tnsr, S_interior_tnsr)\n",
    "\n",
    "# set optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_tnsr)\n",
    "\n",
    "# initialize variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# open session\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "#%% Train network\n",
    "# for each sampling stage\n",
    "for i in range(sampling_stages):\n",
    "    \n",
    "    # sample uniformly from the required regions\n",
    "    t_interior, S_interior, t_terminal, S_terminal = sampler(nSim_interior, nSim_terminal)\n",
    "    \n",
    "    # for a given sample, take the required number of SGD steps\n",
    "    for _ in range(steps_per_sample):\n",
    "        loss,L1,L3,_ = sess.run([loss_tnsr, L1_tnsr, L3_tnsr, optimizer],\n",
    "                                feed_dict = {t_interior_tnsr:t_interior, S_interior_tnsr:S_interior, t_terminal_tnsr:t_terminal, S_terminal_tnsr:S_terminal})\n",
    "    \n",
    "    print(loss, L1, L3, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "efd80e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = tf.constant(0.002, dtype = tf.float64)\n",
    "phi1 = tf.constant(28.0, dtype = tf.float64)\n",
    "phi2 = tf.constant(28.0, dtype = tf.float64)\n",
    "beta1 = tf.constant(0.01, dtype = tf.float64)\n",
    "beta2 = tf.constant(0.01, dtype = tf.float64)\n",
    "eta1 = tf.constant(0.012790328319261378, dtype = tf.float64)\n",
    "eta2 = tf.constant(0.012790328319261378, dtype = tf.float64)\n",
    "a11 = tf.constant(0.014, dtype = tf.float64)\n",
    "a22 = tf.constant(0.013, dtype = tf.float64)\n",
    "alpha = tf.constant(0.1, dtype = tf.float64)\n",
    "scale = np.sqrt(1.754)\n",
    "sigma_k1 = tf.convert_to_tensor(scale*np.array([[.00477,               .0,   .0, .0]]), dtype = tf.float64)\n",
    "sigma_k2 = tf.convert_to_tensor(scale*np.array([[.0              , .00477,   .0, .0]]), dtype = tf.float64)\n",
    "sigma_z =  tf.convert_to_tensor(np.array([[.011*np.sqrt(.5)   , .011*np.sqrt(.5)   , .025, .0]]), dtype = tf.float64)\n",
    "sigma_s =  tf.convert_to_tensor(np.array([[.1*np.sqrt(.5)   , .1*np.sqrt(.5)   , .0, .1]]), dtype = tf.float64)\n",
    "\n",
    "clowerlim = tf.constant(0.0001, dtype = tf.float64)\n",
    "rho = tf.constant(1.0, dtype = tf.float64)\n",
    "gamma = tf.constant(8.0, dtype = tf.float64)\n",
    "kappa = tf.constant(2.0, dtype = tf.float64)\n",
    "zeta = tf.constant(0.5, dtype = tf.float64)\n",
    "wMin = tf.constant(-1., dtype = tf.float64)\n",
    "wMax = tf.constant(1., dtype = tf.float64)\n",
    "zMin = tf.constant(-1., dtype = tf.float64)\n",
    "zMax = tf.constant(1., dtype = tf.float64)\n",
    "vMin = tf.constant(0.001, dtype = tf.float64)\n",
    "vMax = tf.constant(1.999, dtype = tf.float64)\n",
    "\n",
    "# vMin = tf.constant(0.5, dtype = tf.float64)\n",
    "# vMax = tf.constant(1.5, dtype = tf.float64)\n",
    "\n",
    "\n",
    "params = {'delta':delta,\n",
    "         'phi1':phi1,\n",
    "         'phi2':phi2,\n",
    "         'beta1':beta1,\n",
    "         'beta2':beta2,\n",
    "         'eta1':eta1,\n",
    "         'eta2':eta2,\n",
    "         'a11':a11,\n",
    "         'a22':a22,\n",
    "         'alpha':alpha,\n",
    "         'sigma_k1':sigma_k1,\n",
    "         'sigma_k2':sigma_k2,\n",
    "         'sigma_z':sigma_z,\n",
    "         'sigma_s':sigma_s,\n",
    "         'rho':rho,\n",
    "         'gamma':gamma,\n",
    "         'kappa':kappa,\n",
    "         'zeta':zeta,\n",
    "         'wMin':wMin,\n",
    "         'wMax':wMax,\n",
    "         'zMin':zMin,\n",
    "         'zMax':zMax,\n",
    "         'vMin':vMin,\n",
    "         'vMax':vMax,\n",
    "         'clowerlim':clowerlim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d96b55ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def calder(func,W, Z, V):\n",
    "    X = tf.concat([W,Z,V], axis=1)\n",
    "    logXiE       = func(X)\n",
    "    \n",
    "    dW_logXiE     = tf.gradients(logXiE, W)[0];       dZ_logXiE     = tf.gradients(logXiE, Z)[0];       dV_logXiE     = tf.gradients(logXiE, V)[0]; \n",
    "    dW2_logXiE    = tf.gradients(dW_logXiE, W)[0];    dZ2_logXiE    = tf.gradients(dZ_logXiE, Z)[0];    dV2_logXiE    = tf.gradients(dV_logXiE, V)[0];  \n",
    "    dWdZ_logXiE   = tf.gradients(dW_logXiE, Z)[0];    dWdV_logXiE   = tf.gradients(dW_logXiE, V)[0];    dZdV_logXiE   = tf.gradients(dZ_logXiE, V)[0];   \n",
    "    \n",
    "    return dW_logXiE, dZ_logXiE, dV_logXiE, dW2_logXiE, dZ2_logXiE, dV2_logXiE, dWdZ_logXiE, dWdV_logXiE, dZdV_logXiE\n",
    "\n",
    "@tf.function \n",
    "def calvar(func, W, Z, V, params):\n",
    "    X = tf.concat([W,Z,V], axis=1)\n",
    "    logXiE       = func(X)\n",
    "    Vr, Vz, Vs, Vrr, Vzz, Vss, Vrz, Vrs, Vzs = calder(func, W, Z, V)\n",
    "\n",
    "    k1a = (1-params['zeta'] + params['zeta']*tf.exp(W*(1-params['kappa'])))**(1/(params['kappa']-1))\n",
    "    k2a = ((1-params['zeta'])*tf.exp(W*(params['kappa']-1)) + params['zeta'])**(1/(params['kappa']-1))\n",
    "\n",
    "    bb1 = (params['alpha'] - 1/params['phi1']*k1a-1/params['phi2']*k2a)\n",
    "    cc1 = params['delta']*k1a**2/params['phi1']/((1-params['zeta'])*k1a**(1-params['kappa'])-Vr) + params['delta']*k2a**2/params['phi2']/(params['zeta']*(k2a)**(1-params['kappa'])+Vr)\n",
    "    aa1 = -1\n",
    "    sqrt_test1 = bb1**2 - 4*aa1*cc1;\n",
    "    event_A = tf.cast((sqrt_test1 >= 0),tf.float64);\n",
    "\n",
    "    c_root_large = event_A * (-bb1 - tf.sqrt(event_A*sqrt_test1))/(2*aa1);\n",
    "    c_root_small = event_A * (-bb1 + tf.sqrt(event_A*sqrt_test1))/(2*aa1);\n",
    "    c_root_large = c_root_large*tf.cast(tf.math.greater(c_root_large,params['clowerlim']),tf.float64) + \\\n",
    "        params['clowerlim']*tf.ones([batchSize,1],tf.float64)*tf.cast(tf.math.less(c_root_large,params['clowerlim']),tf.float64)\n",
    "\n",
    "    d1_root = 1/params['phi1']-params['delta']*k1a/((1-params['zeta'])*k1a**(1-params['kappa'])-Vr)/params['phi1']/c_root_large\n",
    "    d2_root = 1/params['phi2']-params['delta']*k2a/(params['zeta']*(k2a)**(1-params['kappa'])+Vr)/params['phi2']/c_root_large\n",
    "\n",
    "    uu = params['delta']*tf.math.log(c_root_large)\n",
    "    mu_k1 = (d1_root - params['phi1']/2*d1_root**2) + params['beta1']*Z - eta1\n",
    "    mu_k2 = (d2_root - params['phi2']/2*d2_root**2) + params['beta2']*Z - eta2\n",
    "    mu_r = mu_k2 - mu_k1 - V/2*(tf.reduce_sum(params['sigma_k2']**2) - tf.reduce_sum(params['sigma_k1']**2))\n",
    "\n",
    "    dkadk1dk1 = (kappa-1)*(1-zeta)**2*(k1a)**(-2*kappa+2) - kappa*(1-zeta)*(k1a)**(-kappa+1)\n",
    "    dkadk1dk2 = (kappa-1)*zeta*(1-zeta)*(k1a)**(-kappa+1)*(k2a)**(-kappa+1)\n",
    "    dkadk2dk2 = (kappa-1)*zeta**2*(k2a)**(-2*kappa+2) - kappa*(1-zeta)*(k2a)**(-kappa+1)\n",
    "\n",
    "    mu_1 = mu_k1*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ \\\n",
    "            mu_k2*(params['zeta'])*(k2a)**(1-params['kappa'])+ \\\n",
    "            V/2*(tf.reduce_sum(params['sigma_k1']**2)*dkadk1dk1 + \\\n",
    "                                                tf.reduce_sum(params['sigma_k2']**2)*dkadk2dk2 + \\\n",
    "                                                2*tf.reduce_sum(params['sigma_k1']*params['sigma_k2'])*dkadk1dk2)\n",
    "\n",
    "    zdrift = -params['a11']*Z\n",
    "    sdrift = -params['a22']*(V-1)\n",
    "\n",
    "    h1 = params['sigma_k1'][0,0]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,0]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,0]-params['sigma_k1'][0,0])*Vr + params['sigma_z'][0,0]*Vz+params['sigma_s'][0,0]*Vs\n",
    "    h2 = params['sigma_k1'][0,1]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,1]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,1]-params['sigma_k1'][0,1])*Vr + params['sigma_z'][0,1]*Vz+params['sigma_s'][0,1]*Vs\n",
    "    hz = params['sigma_k1'][0,2]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,2]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,2]-params['sigma_k1'][0,2])*Vr + params['sigma_z'][0,2]*Vz+params['sigma_s'][0,2]*Vs\n",
    "    hs = params['sigma_k1'][0,3]*(1-params['zeta'])*(k1a)**(1-params['kappa'])+ params['sigma_k2'][0,3]*(params['zeta'])*(k2a)**(1-params['kappa'])+\\\n",
    "        (params['sigma_k2'][0,3]-params['sigma_k1'][0,3])*Vr + params['sigma_z'][0,3]*Vz+params['sigma_s'][0,3]*Vs\n",
    "\n",
    "    penalty_term = (1-params['gamma'])*V*(h1**2 + h2**2 + hz**2 + hs**2)\n",
    "    uu = uu + penalty_term + mu_1\n",
    "    t1 = tf.reduce_sum((params['sigma_k2']-params['sigma_k1'])**2)\n",
    "    t2 = tf.reduce_sum((params['sigma_k2']-params['sigma_k1'])*params['sigma_z'])*2\n",
    "    t3 = tf.reduce_sum(params['sigma_z']**2)\n",
    "\n",
    "    t4 = tf.reduce_sum((params['sigma_k2']-params['sigma_k1'])*params['sigma_s'])*2\n",
    "    t5 = tf.reduce_sum(params['sigma_z']*params['sigma_s'])*2\n",
    "    t6 = tf.reduce_sum(params['sigma_s']**2)\n",
    "    higordder = (t1*Vrr + t2*Vrz + t3*Vzz + t4*Vrs + t5*Vzs + t6*Vss)/2*V\n",
    "    \n",
    "    return logXiE, k1a, k2a, c_root_large, d1_root, d2_root, uu, penalty_term, mu_1, mu_k1, mu_k2, mu_r, zdrift, sdrift, h1, h2, hz, hs, higordder, Vr, Vz, Vs, Vrr, Vzz, Vss, Vrz, Vrs, Vzs\n",
    "    \n",
    "@tf.function \n",
    "def HJB_loss_E(func, W, Z, V, params):\n",
    "    X = tf.concat([W,Z,V], axis=1)\n",
    "    logXiE, k1a, k2a, c_root_large, d1_root, d2_root, uu, penalty_term, mu_1, mu_k1, mu_k2, mu_r, zdrift, sdrift, h1, h2, hz, hs, higordder, Vr, Vz, Vs, Vrr, Vzz, Vss, Vrz, Vrs, Vzs = calvar(func, W, Z, V, params)\n",
    "    HJB = uu + mu_r*Vr + zdrift*Vz + sdrift*Vs + higordder\n",
    "    \n",
    "    return HJB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c790dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_factory(model, loss, valueFunctionLogE, W, Z, V, params, loss_type, targets):\n",
    "\n",
    "    ## Obtain the shapes of all trainable parameters in the model\n",
    "    shapes = tf.shape_n(model.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.product(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_model_parameters(params_1d):\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # Create a function that will compute the value and gradient. This can be the function that the factory returns\n",
    "    @tf.function\n",
    "    def val_and_grad(params_1d):\n",
    "        with tf.GradientTape() as tape:\n",
    "          ## Update the parameters in the model\n",
    "            assign_new_model_parameters(params_1d)\n",
    "            ## Calculate the loss \n",
    "            loss_value = loss_type(loss(valueFunctionLogE, W, Z, V, params), targets)\n",
    "        ## Calculate gradients and convert to 1D tf.Tensor\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "        del tape\n",
    "\n",
    "        ## Print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "#         tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        ## Store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "        return loss_value, grads\n",
    "\n",
    "    def f(params_1d):\n",
    "      return [vv.numpy().astype(np.float64)  for vv in val_and_grad(params_1d)]\n",
    "\n",
    "    ## Store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_model_parameters = assign_new_model_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f\n",
    "  \n",
    "\n",
    "## Training step BFGS\n",
    "def training_step_BFGS(valueFunctionLogE, W, Z, V, params, targets, maxiter, maxfun, gtol, maxcor, maxls, ftol):\n",
    "\n",
    "  ## Train experts NN\n",
    "  loss_fun = tf.keras.losses.MeanSquaredError()\n",
    "  func_E = function_factory(valueFunctionLogE, HJB_loss_E, valueFunctionLogE, W, Z, V, params, loss_fun, targets)\n",
    "  init_params_E = tf.dynamic_stitch(func_E.idx, valueFunctionLogE.trainable_variables)\n",
    "\n",
    "  start = time.time()\n",
    "  results = optimize.minimize(func_E, x0 = init_params_E.numpy(), method = 'L-BFGS-B', jac = True, options = {'maxiter': maxiter, 'maxfun': maxfun, 'gtol': gtol, 'maxcor': maxcor, 'maxls': maxls, 'ftol' : ftol})\n",
    "  end = time.time()\n",
    "  print('Elapsed time for experts {:.4f} sec'.format(end - start))\n",
    "  # after training, the final optimized parameters are still in results.position\n",
    "  # so we have to manually put them back to the model\n",
    "  func_E.assign_new_model_parameters(results.x)\n",
    "  \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19d993a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:110: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.W = self.add_variable(\"W\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:114: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.b = self.add_variable(\"b\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:38: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Uz = self.add_variable(\"Uz\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:40: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Ug = self.add_variable(\"Ug\", shape=[self.input_dim ,self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:42: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Ur = self.add_variable(\"Ur\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:44: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Uh = self.add_variable(\"Uh\", shape=[self.input_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:48: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wz = self.add_variable(\"Wz\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:50: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wg = self.add_variable(\"Wg\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:52: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wr = self.add_variable(\"Wr\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:54: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.Wh = self.add_variable(\"Wh\", shape=[self.output_dim, self.output_dim],\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:58: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bz = self.add_variable(\"bz\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:59: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bg = self.add_variable(\"bg\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:60: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.br = self.add_variable(\"br\", shape=[1, self.output_dim])\n",
      "/scratch/local/jobs/2667634/ipykernel_1474227/1937691871.py:61: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  self.bh = self.add_variable(\"bh\", shape=[1, self.output_dim])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Elapsed time for experts 15.1449 sec\n",
      "717.9051090262733\n",
      "Iteration 1\n",
      "Elapsed time for experts 5.5410 sec\n",
      "1580.8670947599035\n",
      "Iteration 2\n",
      "Elapsed time for experts 12.1436 sec\n",
      "1886.3129482562683\n",
      "Iteration 3\n",
      "Elapsed time for experts 7.0127 sec\n",
      "74.36334004118048\n",
      "Iteration 4\n",
      "Elapsed time for experts 6.6890 sec\n",
      "43.14567341777585\n",
      "Iteration 5\n",
      "Elapsed time for experts 5.6291 sec\n",
      "89.16303056181617\n",
      "Iteration 6\n",
      "Elapsed time for experts 31.6583 sec\n",
      "65.30235248701678\n",
      "Iteration 7\n",
      "Elapsed time for experts 32.9455 sec\n",
      "13.620537406461091\n",
      "Iteration 8\n",
      "Elapsed time for experts 8.8176 sec\n",
      "0.41553833479296787\n",
      "Iteration 9\n",
      "Elapsed time for experts 10.7370 sec\n",
      "0.011608658463293654\n",
      "Iteration 10\n",
      "Elapsed time for experts 6.0891 sec\n",
      "0.19194206391864843\n",
      "Iteration 11\n",
      "Elapsed time for experts 24.8505 sec\n",
      "0.007556416236591981\n",
      "Iteration 12\n",
      "Elapsed time for experts 5.8969 sec\n",
      "4.856075533520097\n",
      "Iteration 13\n",
      "Elapsed time for experts 28.2763 sec\n",
      "0.19883034704616176\n",
      "Iteration 14\n",
      "Elapsed time for experts 8.5637 sec\n",
      "0.1571199583286581\n",
      "Iteration 15\n",
      "Elapsed time for experts 29.7231 sec\n",
      "0.006178814130812389\n",
      "Iteration 16\n",
      "Elapsed time for experts 15.9658 sec\n",
      "0.22694501152382024\n",
      "Iteration 17\n",
      "Elapsed time for experts 6.2406 sec\n",
      "0.03904855570031099\n",
      "Iteration 18\n",
      "Elapsed time for experts 11.7106 sec\n",
      "0.060081017346760766\n",
      "Iteration 19\n",
      "Elapsed time for experts 10.0993 sec\n",
      "0.026725395953570298\n",
      "Iteration 20\n",
      "Elapsed time for experts 8.4550 sec\n",
      "0.1779438712193418\n",
      "Iteration 21\n",
      "Elapsed time for experts 12.8226 sec\n",
      "0.009464740826127433\n",
      "Iteration 22\n",
      "Elapsed time for experts 11.1306 sec\n",
      "0.024987083337235455\n",
      "Iteration 23\n",
      "Elapsed time for experts 6.9455 sec\n",
      "0.03388981819162098\n",
      "Iteration 24\n",
      "Elapsed time for experts 31.6327 sec\n",
      "0.02278776940464181\n",
      "Iteration 25\n",
      "Elapsed time for experts 13.2683 sec\n",
      "0.0411487605015472\n",
      "Iteration 26\n",
      "Elapsed time for experts 5.8064 sec\n",
      "5.41298187944945\n",
      "Iteration 27\n",
      "Elapsed time for experts 7.3333 sec\n",
      "0.06438139471805139\n",
      "Iteration 28\n",
      "Elapsed time for experts 21.0994 sec\n",
      "0.11995785967438304\n",
      "Iteration 29\n",
      "Elapsed time for experts 29.0085 sec\n",
      "0.042318276070302314\n",
      "Iteration 30\n",
      "Elapsed time for experts 7.9777 sec\n",
      "0.7303861769622588\n",
      "Iteration 31\n",
      "Elapsed time for experts 21.1053 sec\n",
      "0.06600540846109637\n",
      "Iteration 32\n",
      "Elapsed time for experts 15.7161 sec\n",
      "0.013289553504797727\n",
      "Iteration 33\n",
      "Elapsed time for experts 12.7761 sec\n",
      "0.024334358593838937\n",
      "Iteration 34\n",
      "Elapsed time for experts 7.5980 sec\n",
      "0.06463295174641542\n",
      "Iteration 35\n",
      "Elapsed time for experts 5.9756 sec\n",
      "0.0513341608294073\n",
      "Iteration 36\n",
      "Elapsed time for experts 24.0072 sec\n",
      "0.02852398474208619\n",
      "Iteration 37\n",
      "Elapsed time for experts 24.7500 sec\n",
      "0.021257150016924693\n",
      "Iteration 38\n",
      "Elapsed time for experts 15.4979 sec\n",
      "0.022220309904856638\n",
      "Iteration 39\n",
      "Elapsed time for experts 12.8713 sec\n",
      "0.003558477786259958\n",
      "Iteration 40\n",
      "Elapsed time for experts 29.5428 sec\n",
      "9.511495870602364\n",
      "Iteration 41\n",
      "Elapsed time for experts 6.7433 sec\n",
      "0.016976330008441797\n",
      "Iteration 42\n",
      "Elapsed time for experts 20.0051 sec\n",
      "0.004597741545307315\n",
      "Iteration 43\n",
      "Elapsed time for experts 13.1218 sec\n",
      "0.0025217900912334968\n",
      "Iteration 44\n",
      "Elapsed time for experts 22.3865 sec\n",
      "0.002139370004665054\n",
      "Iteration 45\n",
      "Elapsed time for experts 16.0285 sec\n",
      "0.0006693639579402353\n",
      "Iteration 46\n",
      "Elapsed time for experts 5.4521 sec\n",
      "2.1121450636984562\n",
      "Iteration 47\n",
      "Elapsed time for experts 17.9086 sec\n",
      "0.03301700304369646\n",
      "Iteration 48\n",
      "Elapsed time for experts 24.0165 sec\n",
      "0.009377863264115792\n",
      "Iteration 49\n",
      "Elapsed time for experts 19.1388 sec\n",
      "0.011801131066134224\n",
      "Elapsed time for training 757.4609 sec\n"
     ]
    }
   ],
   "source": [
    "points_size = 1\n",
    "dimension = 3\n",
    "units = 16\n",
    "activation = 'tanh'\n",
    "kernel_initializer = 'glorot_normal'\n",
    "batchSize = 2048 * points_size\n",
    "batchSize = 100\n",
    "iter_num = 50\n",
    "BFGS_maxiter  = 1000\n",
    "BFGS_maxfun   = 1000\n",
    "BFGS_gtol     = 1.0 * np.finfo(float).eps\n",
    "BFGS_maxcor   = 100\n",
    "BFGS_maxls    = 100\n",
    "BFGS_ftol     = 1.0 * np.finfo(float).eps\n",
    "\n",
    "## NN structure\n",
    "tf.keras.backend.set_floatx(\"float64\") ## Use float64 by default\n",
    "\n",
    "# logXiE_NN = tf.keras.Sequential(\n",
    "#     [tf.keras.Input(shape=[dimension,]),\n",
    "#       tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "#       tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "# #       tf.keras.layers.Dense(units, activation=activation, kernel_initializer=kernel_initializer),\n",
    "#       tf.keras.layers.Dense(1,  activation= None,  kernel_initializer='glorot_normal')])\n",
    "logXiE_NN = DGMNet(layer_width =16, n_layers = 3, input_dim = 3, final_trans=None)\n",
    "\n",
    "start = time.time()\n",
    "targets = tf.zeros(shape=(batchSize,1), dtype=tf.float64)\n",
    "for iter in range(iter_num):\n",
    "  W = tf.random.uniform(shape = (batchSize,1), minval = params['wMin'], maxval = params['wMax'], dtype=tf.float64)\n",
    "  Z = tf.random.uniform(shape = (batchSize,1), minval = params['zMin'], maxval = params['zMax'], dtype=tf.float64)\n",
    "  V = tf.random.uniform(shape = (batchSize,1), minval = params['vMin'], maxval = params['vMax'], dtype=tf.float64)\n",
    "  print('Iteration', iter)\n",
    "  results = training_step_BFGS(logXiE_NN, W, Z, V, params, targets,\\\n",
    "                    maxiter = BFGS_maxiter, maxfun = BFGS_maxfun, gtol = BFGS_gtol, maxcor = BFGS_maxcor, maxls = BFGS_maxls, ftol = BFGS_ftol)\n",
    "  print(results.fun)\n",
    "  if results.fun< 1e-11:\n",
    "    break\n",
    "end = time.time()\n",
    "training_time = '{:.4f}'.format((end - start)/60)\n",
    "print('Elapsed time for training {:.4f} sec'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c878cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logXiE, k1a, k2a, c_root_large, d1_root, d2_root, uu, penalty_term, mu_1, mu_k1, mu_k2, mu_r, zdrift, sdrift, h1, h2, hz, hs, higordder, Vr, Vz, Vs, Vrr, Vzz, Vss, Vrz, Vrs, Vzs\\\n",
    "= calvar(logXiE_NN, np.linspace(-1,1,batchSize).reshape([batchSize,1]), np.zeros([batchSize,1]), np.ones([batchSize,1]), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "063af5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('/project/lhansen/twocapstk/output/twocap_stk_111_all_cov_demean/llim_1.0_lscale_0.1_zscale_0.1/sscale_1.0_srange_0.999/kappa_'+str(kappa.numpy())+'_zeta_'+str(zeta.numpy())+\\\n",
    "        '/gamma_'+str(gamma.numpy())+'_rho_'+str(rho.numpy())+'/model_sym_5.0e-5.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc1370ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid points for l:  101\n",
      "grid size for l:  0.02\n",
      "grid points for z:  21\n",
      "grid size for z:  0.1\n",
      "grid points for s:  21\n",
      "grid size for s:  0.0999\n"
     ]
    }
   ],
   "source": [
    "lscale = 0.1\n",
    "zscale = 0.1\n",
    "sscale = 1.0\n",
    "llim = 1.0\n",
    "srange = 0.999\n",
    "\n",
    "lgrid = int(1000*lscale+1)\n",
    "print('grid points for l: ', lgrid)\n",
    "rscale = llim*2/(lgrid-1)\n",
    "print('grid size for l: ', rscale)\n",
    "\n",
    "zgrid = int(200*zscale+1)\n",
    "print('grid points for z: ', zgrid)\n",
    "zscale = 2/(zgrid-1)\n",
    "print('grid size for z: ', zscale)\n",
    "\n",
    "sgrid = int(20*sscale+1)\n",
    "print('grid points for s: ', sgrid)\n",
    "sscale = 2*srange/(sgrid-1)\n",
    "print('grid size for s: ', sscale)\n",
    "\n",
    "ll = np.linspace(-llim,llim,lgrid)\n",
    "zz = np.linspace(-1,1,zgrid)\n",
    "ss = np.linspace(-srange+1,srange+1,sgrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60e83a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/python-anaconda-2022.05-el8-x86_64/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "/software/python-anaconda-2022.05-el8-x86_64/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (6,6))\n",
    "sns.lineplot(np.linspace(-1,1,batchSize),c_root_large.numpy().flatten(),label = \"NN\")\n",
    "sns.lineplot(np.linspace(-1,1,lgrid),npz['cons'][:,10,10],label = \"FDM\")\n",
    "\n",
    "ax.set_ylim([-0.01,0.3])\n",
    "ax.set_ylabel(r'$c$')\n",
    "ax.set_xlabel(r'$\\hat{y}$')\n",
    "ax.set_title(r'c, '+ '$\\gamma=$'+str(gamma.numpy())+', '+'$\\\\rho$'+'='+str(rho.numpy())+', '+'$\\\\kappa$'+'='+str(kappa.numpy()))\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "# fig.savefig(figname+'/con.png', dpi = 400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50356a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b454fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape2nd:\n",
    "    with tf.GradientTape() as tape:\n",
    "      logVF = logXiE_NN(X)\n",
    "      tape.watch(X)\n",
    "    dVdX = tape.gradient(logVF, X)\n",
    "    tape2nd.watch(X)\n",
    "# with tf.GradientTape() as tape:\n",
    "#   logVF = logXiE_NN(X)\n",
    "#   tape.watch(X)\n",
    "# dVdX = tape.gradient(logVF, X)\n",
    "    ddVddX = tape2nd.gradient(dVdX, X)\n",
    "Vr = tf.reshape(dVdX[:,0],[batchSize,1])\n",
    "Vz = tf.reshape(dVdX[:,1],[batchSize,1])\n",
    "Vs = tf.reshape(dVdX[:,2],[batchSize,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead86f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
